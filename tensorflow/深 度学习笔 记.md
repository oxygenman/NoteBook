#                               深 度学习笔 记

## 一、经典深度网络模型

1. [Deep Learning 回顾之LeNet,AlexNet,GoogLeNet,VGG,ResNet](https://www.cnblogs.com/52machinelearning/p/5821591.html)

2. [LeNet5神经网络简介及TensorFlow实现](https://blog.csdn.net/roguesir/article/details/73770448)

3. [Inception in CNN](https://blog.csdn.net/stdcoutzyx/article/details/51052847)

4. [LocalResponseNormalization](https://www.quora.com/What-is-local-response-normalization)

5. [VGGnet](https://blog.csdn.net/muyiyushan/article/details/62895202) [VGG代码](https://www.cnblogs.com/hellcat/p/8036300.html)  [VGGnet介绍](https://blog.csdn.net/marsjhao/article/details/72955935) 

6. [googlenet Inception](https://blog.csdn.net/yuanchheneducn/article/details/53045551) 

7. [残差网络ResNet](https://blog.csdn.net/lanran2/article/details/79057994) 

   ​

## 二、BatchNormalization

1. [白化](https://blog.csdn.net/hjimce/article/details/50864602) 白化的目的是去除数据的冗余信息，经过白化后的数据满足两个性质:(1)特征之间相关性较低（2）所有特征具有相同的方差。分为PCA白化和ZCA白化。
2. [BatchNormalization](https://blog.csdn.net/ZhikangFu/article/details/53391840)  [BatchNormalization 知乎](https://www.zhihu.com/question/38102762)   

## 三、网络压缩

[CNN网络优化学习总结](https://blog.csdn.net/sun_28/article/details/78170878)

[超全总结:神经网络加速之量化模型](https://www.sohu.com/a/233688290_500659)

现阶段，在建立小型高效的神经网络工作中，通常可分为两类工作：

- **压缩预训练模型** 量化压缩（product quantization)、哈希（hashing）、剪枝（pruning）、矢量编码（vector quantization）和霍夫曼编码（huffman coding）；此外还有各种分解因子（various factorization）用来加速预训练网络；还有一种使用大型网络指导训练小型网络，叫做蒸馏（distillation）。
- [深度学习网络压缩模型方法总结](https://www.cnblogs.com/zhonghuasong/p/7493475.html)
- **直接训练小型模型** 直接设计新的网络结构，用较少的参数达到大网络相同的效果。如Flattened networks,Factorized networks, xcepption network, resneXt,squeezenet, mobile net, shuffle net;

### 1.Mobile net v1

mobilenetv1主要有两方面的工作，（1）基于深度可分离卷积的网络结构（2）两个全局超参数width multiplier和resolution multiplier.

(1)深度可分离卷积最早是由google brain的一名实习生Laurent Sifre于2013年提出[1]。深度可分离卷积把标准卷积分解成深度卷积（depthwise convolution）和 逐点卷积（pointwise convolution)。这么做的好处是可以大幅度降低参数量和计算量。

传统的卷积核采用和输入数据通道数相同的通道数即

$M\times D_{k} \times D_{k}$

一个卷积核处理数据的计算量为：

$M\times D_{k} \times D_{k} \times D_{F} \times D_{F}$

在某一层如果使用N个卷积核，这一卷积层的计算量为：

$M\times D_{k} \times D_{k} \times D_{F} \times D_{F} \times N$

如果使用depth-wish方式的卷积核，我们会首先使用一组二维的卷积核，也就是卷积核的通道数为1，这一组二维卷积核的数量和输入通道数相同。

在使用逐个通道卷积处理之后，再使用3D的1*1卷积核来处理之前输出的特征图，将最终的通道数变为一个指定的数量。

从理论上俩看，这种方式的运算量为：

其中，一组和输入通道数相同的2D卷积核的运算量为：

$D_{K} \times D_{K} \times M \times D_{F} \times D_{F}$

1*1卷积核的计算量为：

$N \times M \times D_{F} \times D_{F}$

因此这种组合方式的计算量为：

depth-wise方式的卷积相比于传统3D卷积计算量为：

$\frac{1}{N} + \frac{1}{D^{2}_k}$



![img](https://ws2.sinaimg.cn/large/006tNbRwly1fyj6szn6bvj30fi0jbwf1.jpg)

网络结构

传统卷积的网络结构使用的方式如左侧所示，deep-wise卷积方式如下图右边所示。

  ![img](https://ws2.sinaimg.cn/large/006tNbRwly1fyj80lm39ej30fp09rweo.jpg)

[GEMM caffe中卷积的实现](https://xmfbit.github.io/2017/03/08/mathfunctions-in-caffe/)

MobileNet在ImageNet上训练时使用的网络结构

![img](https://ws4.sinaimg.cn/large/006tNbRwly1fyj9w00p99j30ka0no41m.jpg)

下图为MobileNet对于不同结构单元在计算量和参数数量方面的统计。

![img](https://ws1.sinaimg.cn/large/006tNbRwly1fyj9x9bv7qj30gf07c0sk.jpg)

（2）两个全局超参数 width multiplier 和 resolution multiplier

尽管标准的MobileNet在计算量和模型尺寸方面具备了很明显的优势，但是，在一些运行速度或内存有极端要求的场合，还需要更小更快的模型，如何能够在不重新设计模型 的情况下，以最小的改动就可以获得更小更快的模型，本文中宽度因子和分辨率因子就是解决这些问题的配置参数。

宽度因子$\alpha$ 是一个属于（0，1]之间的数，附加于网络的通道数。简单来说就是新网络中每一个模块要使用的卷积核数量相较于标准MobileNet比例.对于depth-wise结合1x1方式的卷积核，计算量为：

$D_k \times D_k \times \alpha N\times \alpha M\times D_F \times D_F + \alpha M \times D_F \times D_F$

$\alpha$ 常用的配置为1，0.75，0.5，0.25；当$\alpha$ 等于1时就是标准的MobileNet.通过参数$\alpha$ 可以非常有效的将计算量和参数数量约减到$\alpha$ 的平方倍。

分辨率因子$\beta$ 的取值范围在（0，1）之间，是作用于每一个模块输入尺寸的约减因子，简单来说就是将输入数据以及由此在每一个模块儿产生的特征图都变雄安，结合$\alpha$ 和 $\beta$ 的计算量为：

$D_k \times D_k \times \alpha N\times \alpha M\times \beta D_F \times \beta D_F + \alpha M \times \beta D_F \times \beta D_F$

### 2.MobileNet v2

《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation 》于2018年1月公开在arXiv(美[ˈɑ:rkaɪv]) ：https://arxiv.org/abs/1801.04381 

MobileNetv2 是对MobileNetV1的改进，同样是一个轻量化卷积神经网络。

mobilenet v2的创新点儿有两个，

一个是inverted residuals ,通常的residuals block是先经过一个1*1的 conv layer, 把feture map的通道数压下来，再经过3 * 3的conv layer, 最后经过一个1 * 1的conv layer, 将feature map 通道数扩张回去，即先压缩，最后扩张回去。而inverted residuals 就是先扩张，最后压缩。

二 是 Linear bottlenecks ,为了避免Relu对特征的破坏，在residual block的 Eltwise sum 之前的那个1*1conv 不再采用relu.

首先看和mobilenetV1的区别：

![mobilenetv1vsmobilenetv2](https://ws3.sinaimg.cn/large/006tNbRwly1fyjbv4lsbwj30sr09xaab.jpg)

主要是两点：

1.Depth-wise convolution 之前多了一个1*1的扩张层，目的是为了提升通道数，获得更多特征；

2.最后不采用relu,而是linear,目的是防止relu 破坏特征。

再看mobilenetv2的block 与 resnet的block:

![mobilenetv2与resnet](https://ws4.sinaimg.cn/large/006tNbRwly1fyjc3ke2z6j30wk09hq37.jpg)

主要不同之处就在于，resnet是：压缩-卷积特征提取-扩张，mobilenetv2则是inverted reiduals,即扩张-卷积提特征-压缩。

直接把depth-wise separable convolution 应用到residual block中，会碰到如下问题：

1.DWConv layer 层提取到的特征受限于输入通道数，若是采用以往的residual block,先压缩，再卷积提取特征，那么DWconv layer 可提取的卷积特征就太少了，mobilenetv2反其道而行，一开始先扩张，本文实验扩张倍数为6.因此称作inverted residual .

2.当采用扩张-卷积提取特征-压缩时，在压缩之后会碰到一个问题，那就是relu会破坏特征。为什么relu会破坏特征呢？

这要从relu的性质说起relu对于负的特征，输出全为0；而本来特征就已经被压缩，再经过relu的话，又要损失一部分特征，因此这里不采用relu,实验结果表明这样做是正确的，这就称为linear bottlenecks

github开源源码： 
https://github.com/suzhenghang/MobileNetv2/tree/master/.gitignore 
另外一位朋友的prototxt： 

https://github.com/austingg/MobileNet-v2-caffe

### 3.[ResNeXt](https://blog.csdn.net/zziahgf/article/details/78854456)

ResNeXt主要引入了Cardinality,作为网络depth和width维度之外的一种必要因子。

实验结果表明，增加Cardinality比加深或者加宽网络结构更有效。

ResNeXt采用聚合变换的思想

aggregated transformations表示为：

$F(x)=\sum_{i=1}^{C}\tau (x)$

其中，$\tau(x)$可以是任意函数，其将x投影到一个嵌入空间（一般是低维空间），并进行变换。

C是待聚合的变换集的大小，即Cardinality.类似于D。

Cardinality维度决定了复杂变换的数目。

对于变换函数的设计，采用策略是：所有$\tau_{i}$拓扑结构相同。

基于residual函数的aggregated transformation:
$y=x+\sum_{i=1}^{C}\tau _{i}(x)$

![è¿éåå¾çæè¿°](https://ws3.sinaimg.cn/large/006tNbRwly1fykj552rn4j30jz0bvwfi.jpg)

![è¿éåå¾çæè¿°](https://ws1.sinaimg.cn/large/006tNbRwly1fykj6da8v0j315c0ewtb8.jpg)

在group方式中，宽度较宽。作者实验表明Cardinality参数对于网络的提升作用是绝对的，也就是说不管网络如何设定超参数，只要提高cardinality必然能够提高网络的效果。

### 4.shuffleNet

shuffleNet相比MobileNet提高了准确率，凸显了分组卷积在运算效率上的优势。

首先：Xception与ResNext所引入的depthwise separable convolution和group convolution 虽然能协调模型的能力与计算量，但被发现它们的pointwise convolution 占据着很大的计算量，因此shuffleNet 考虑引入pointwise group convolution来解决这个问题。既然ResNeXt在瓶颈模块儿中间采用了splitting策略，为何就不再输入就采用这种策略呢？这样网络不就整体分离了吗？然而，这个分组g跟cardinality一样影响着模型的能力，由此pointwise group convolution带来了刺激问题。而这个问题的本质是什么呢？对比分组卷积和常规卷积的运算规则，我们能够发现根本矛盾可能是分组卷积没有Channel Correlation,Facee++ 用 Channel Shuffle 来解决这个问题。

![è¿éåå¾çæè¿°](https://ws2.sinaimg.cn/large/006tNbRwly1fylkg5brpqj30uo0gqjt6.jpg)

如上图所示，input分成3组并分别作了对应的变换（3x3 GConv1),然后在下一次变换（3x3  GConv2）之前做了一次分组间的Channel Shuffle.如此一来，每个分组就含有了其它分组的局部 Channel Correlation了。如果Channel Shuffle 次数足够多，我觉着就可以认为这完全等效于常规卷积运算了。

ShuffleNet 以Channel Shuffle为基础构造出ShuffleNet Unit:	

![è¿éåå¾çæè¿°](https://ws4.sinaimg.cn/large/006tNbRwly1fylkzbv1fvj30u00w041v.jpg)

对于上图Figure2(a)的ResNet瓶颈模块儿，比如输入大小为cxhxw与瓶颈channels数为m的情况，ResNet模块的计算量为

$hw(2cm+9m^2)$Flops,ResNeXt模块的计算量为$hw(2cm+9m^2/g)$FLOPS.而ShuffleNet模块是hw(2cm/g+9m)Flops,其中g代表分组数。可以看出，ShuffleNet确实比ResNext要少些计算量。

### 5.[XNOR-Net：IMageNet Classification Using Binary Convolutional Neural Networks](https://blog.csdn.net/ajj15120321/article/details/80799261)

本文是一篇经典的二值化weight和activations的文章，发表在ECCV2016.本文提出两种有效二值化框架：XNOR-Net以及BWN(Binary-Weight-Networks).在存储方面可以节省32倍的memory,在XNOR-Net上weights以及卷积层的input都是二值化的。

1.Binary-Weight-Networks(BWN)

本文直接给出了真实值W与目标二值化$B \in\{+1,-1\}^{c\times w\times h}$之间的关系，用一个scaling factor $\alpha \in \mathbb{R}^+$来联系两个weight。$W \approx \alpha B$,一个卷积操作就可以近似为

$I*W \approx (I\oplus B)\alpha$   (1 )

$\oplus$ 代表不带任何乘法的卷积操作。由于weight是二值化的，所以可以实现一个卷积操作转化为加法和减法。用$<I,B,A,\oplus>$ 代表二值化weight的cnn,$B=B_{lk}$是一个二值化的filter,$\alpha = A_{lk}, W_{lk} \approx A_{lk}B_{lk}$

Estimating binary weights

为了让loss不是一般性，假设$W，B \in \mathbb{R}^n, n = c\times w \times h$都是矢量，对于$ W\approx \alpha B$,为了找到最优解，本文提出以下优化目标函数：

$J(B,\alpha) = ||W-\alpha B||^2$(1)

$\alpha ^*, B^* = \arg\underset{\alpha ,B}{\min}J(B,\alpha)$(2)  

展开公式（2）

$J(B,\alpha)=\alpha^2B^TB - 2\alpha W^TB + W^TW$(3)

$B\in \{+1,-1\}^n, B^TB = n$是一个常数。$W^TW$也是一个常数，由于W是一个已经得到的变量，设定$c=W^TW$,这样公式（3）就变为：

$J(B,\alpha) = \alpha^2n - 2\alpha W^TB + c$

这样求B的最优解就变为以下的约束公式：

$B^* = arg\underset{B}{max}, s.t. B\in \{+1,-1\}^n$(4)

这个最优解就是$B_i = +1 ifW_i\geq0 以及B_i = -1if W_i < 0 $,因此最优解是：$B^* = sign(W)$,为了找到$\alpha ^*$的最优解，可以求J对$\alpha$的导数并置零得：

$\alpha ^* =\frac{W^TB^*}{n}$(5)

带入B* = sign(W)得：

$\alpha^* = \frac{W^Tsign(W)}{n}=\frac{\sum|W_i|}{n}=\frac{1}{n}||W||_{l1}$(6) 

借鉴Yoshua Benjio 在2013年的时候通过“straight-through estimator”随机离散化神经元来进行梯度计算和反向传播工作，提出了一种考虑激活函数饱和死区特性的“straight-through estimator”, 简单的就是做了如稀土的处理。使得函数在[-1,1]之是可导的，且梯度恒为1.



![img](https://ws1.sinaimg.cn/large/006tNbRwly1fyosfsdoehj30h309paa6.jpg)

Training Binary-weights -Networks



![这里写图片描述](https://ws1.sinaimg.cn/large/006tNbRwly1fynxyx91vlj30q90eb0u3.jpg)

 XNOR-Network

在BWN里面，用A和B来近似表示真实值，但是卷积蹭的输入仍然是真实值的。

1.Binary Dot Product

![   这里写图片描述](https://ws1.sinaimg.cn/large/006tNbRwly1fyowq01sadj30qa0eygmy.jpg)

输入$I \in \mathbb { R } ^ { c \times w _ { i n } \times h _ { i n } }$ 在图二有两个sub_tensors$X_1$和$X_2$。由于sub_tensors 之间有很多重叠，导致很多冗余计算。为了克服这个冗余，首先计算出输入I在channel的均方和，$A = \frac { \sum \left| I _ { i:,:, i } \right| } { c }$ 将A和一个2D的卷积核$k\in \mathbb{R}^{w\times h},K = A * k , k _ { i j } = \frac { 1 } { w \times h },\forall i j , K _ { i j }$ 对应ij处的$\beta$,一旦得到$\alpha$和$\beta$ :

$I * W \approx ( \operatorname { sign } ( I ) \otimes \operatorname { sign } ( W ) ) \odot K \alpha$ (11

$\otimes$表示一个卷积操作使用XNOR和bitcount操作。

[DeepCompression](https://blog.csdn.net/may0324/article/details/52935869)

详情请看博客，不再一一码字。

[Distilling the knowledge in a Neural Network](https://blog.csdn.net/zjc8888888888/article/details/80641751)

这篇文章是Hinton2015年发的，对于几乎所有的机器学习算法，一种简单的提高性能的方法，就是使用同样的数据集训练多个不同的模型，测试时取他们的各自预测值的加权平均作为整个算法的最终输出结果。然而，这样做的缺点也是非常明显的，多个模型的集合体积庞大，且运算需求极大，难以部署在大量用户的机器上。

因此，本文主要做出了一下两点贡献：
1.提出一种知识蒸馏（knowledge distillation）方法，从大模型所学习到的知识中学习有用信息来训练小模型。在保证性能差不多的情况下进行模型压缩。

2.提出一种新的集成模型（Ensembles of Models）方法，包括一个通用模型（Generalist Model）和 多个专用模型（Specialist Models）,其中，专用模型用来对那些通用模型无法区分的细粒度（Fine-grained）类别的图像进行区分。







## 参考文献

【1】L. Sifre. Rigid-motion scattering for image classification.PhD thesis, Ph. D. thesis, 2014. 1, 3

## 

## 