## 张量分解算法详解

[浅谈张量分解（二）：张量分解的数学基础](https://zhuanlan.zhihu.com/p/24824550)

[矩阵乘法和向量乘法](https://zhuanlan.zhihu.com/p/79760117)

[张量分解](https://blog.csdn.net/bqw18744018044/article/details/104948578)

[低秩分解](https://www.jianshu.com/p/e190ec759968)

### 1.基础知识

#### 向量的外积（vector outer product）

给定向量![[公式]](https://www.zhihu.com/equation?tex=%5Cvec+a%3D%5Cleft%28+1%2C2+%5Cright%29+%5E%7BT%7D+)，向量![[公式]](https://www.zhihu.com/equation?tex=%5Cvec+b%3D%5Cleft%28+3%2C4+%5Cright%29+%5E%7BT%7D+)，则![[公式]](https://www.zhihu.com/equation?tex=%5Cvec+a%5Ccirc+%5Cvec+b%3D%5Cvec+a%5Cvec+b%5E%7BT%7D%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+3+%26+4+%5C%5C+6+%26+8+%5C%5C+%5Cend%7Barray%7D+%5Cright%5D)，运算符号“![[公式]](https://www.zhihu.com/equation?tex=%5Ccirc+)”表示外积。另给定向量![[公式]](https://www.zhihu.com/equation?tex=%5Cvec+c%3D%5Cleft%28+5%2C6%2C7+%5Cright%29+%5E%7BT%7D+)，若![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cmathcal%7BX%7D%7D%3D%5Cvec+a%5Ccirc+%5Cvec+b%5Ccirc+%5Cvec+c)，则

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cmathcal%7BX%7D%7D%5Cleft%28+%3A%2C%3A%2C1%5Cright%29+%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+1%5Ctimes+3%5Ctimes+5+%26+1%5Ctimes+4%5Ctimes+5+%5C%5C+2%5Ctimes+3%5Ctimes+5+%26+2%5Ctimes+4%5Ctimes+5+%5C%5C+%5Cend%7Barray%7D+%5Cright%5D%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+15+%26+20+%5C%5C+30+%26+40+%5C%5C+%5Cend%7Barray%7D+%5Cright%5D)，

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cmathcal%7BX%7D%7D%5Cleft%28+%3A%2C%3A%2C2%5Cright%29+%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+1%5Ctimes+3%5Ctimes+6+%26+1%5Ctimes+4%5Ctimes+6+%5C%5C+2%5Ctimes+3%5Ctimes+6+%26+2%5Ctimes+4%5Ctimes+6+%5C%5C+%5Cend%7Barray%7D+%5Cright%5D%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+18+%26+24+%5C%5C+36+%26+48+%5C%5C+%5Cend%7Barray%7D+%5Cright%5D)，

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cmathcal%7BX%7D%7D%5Cleft%28+%3A%2C%3A%2C3%5Cright%29+%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+1%5Ctimes+3%5Ctimes+7+%26+1%5Ctimes+4%5Ctimes+7+%5C%5C+2%5Ctimes+3%5Ctimes+7+%26+2%5Ctimes+4%5Ctimes+7+%5C%5C+%5Cend%7Barray%7D+%5Cright%5D%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+21+%26+28+%5C%5C+42+%26+56+%5C%5C+%5Cend%7Barray%7D+%5Cright%5D)，

其中，![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cmathcal%7BX%7D%7D)是一个三维数组（有三个索引），对于任意索引![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28+i%2Cj%2Ck+%5Cright%29+)上的值为![[公式]](https://www.zhihu.com/equation?tex=x_%7Bijk%7D%3Da_i%5Ccdot+b_j%5Ccdot+c_k%2Ci%3D1%2C2%2Cj%3D1%2C2%2Ck%3D1%2C2%2C3)，在这里，向量![[公式]](https://www.zhihu.com/equation?tex=%5Cvec+a), ![[公式]](https://www.zhihu.com/equation?tex=%5Cvec+b), ![[公式]](https://www.zhihu.com/equation?tex=%5Cvec+c)的外积即可得到一个第三阶张量（third-order tensor），如图1所示。

![img](https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/v2-3847e5e46bc6938dc1c1f08fa1b1bd6c_720w.png)

#### 秩一张量？

若张量$\mathcal{A} \in R^{n_{1} \times \ldots \times n_{d}}$ 可以被写成$d$ 个向量的外积时，则$rank(\mathcal{A})=1$ ,其中符号“rank”表示张量的秩。

那么为了更好的理解，我们可以先来回忆一下矩阵SVD的概念：

![img](https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/v2-9edbfe437885599f04556b23d3fe770e_720w.jpg)

上图是矩阵SVD分解的示意图，一个矩阵可以分解为左右奇异向量矩阵以及一个奇异值矩阵的乘积形式，我们现在不妨以另外一个角度看待矩阵SVD分解：

我们如果拿出第一个左奇异向量以及第一个右奇异向量，这两个向量做外积，我们就可以得到一个矩阵，同时这两个奇异向量对应同一个奇异值，我们尝试将奇异值理解为这两个向量外积得到这个矩阵在原始矩阵中所占的权重，以此类推我们就可以得到所有奇异值对应的左右奇异向量外积的结果矩阵，然后将这些矩阵加起来就得到了原是矩阵。我们把奇异值放到外积的结果矩阵中，并将这种带有权重的矩阵称作因子矩阵，那么矩阵SVD分解就是将矩阵分解为许多因子矩阵之和。

那么这里边的每一个矩阵分量，都是一个秩一矩阵，推广到高维就是秩一张量。

### 2.cp分解

所以我们可以将CP分解看做是矩阵SVD分解的高阶扩展。所以CP分解就是将张量分解为一系列秩一张量之和，即许多因子张量之和。从而引出张量CP分解的定义：

对于：$\mathcal{X}\in\mathcal{R}^{I\times J \times K}$ ,其CP分解公式如下：

$\mathcal{X} \approx \sum_{r=1}^{R} \mathbf{a}_{r} \circ \mathbf{b}_{r} \circ \mathbf{c}_{r}$ 

注意 $a_r, b_r, c_r$的维度分别是$a_r\in R^I,b_r \in R^J ,c_r \in R^K$ , R就是我们所说的CP秩，

![img](https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/v2-1a2cb0847d2fbe6b85a4f6728025e943_720w.jpg)

为了便于理解，我么举个例子：

假设$I=J=K=3$ ,那么$a \circ b \circ c$如下所示：

![img](https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/v2-f89f5b5df9546c01400e0f0b29e0bfaf_720w.jpg)

![img](https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/v2-23d1a58bc21fdadeec765e2943ff3e4e_720w.jpg)

也就是说，第一个因子张量的左上角位置的元素为$a_{11}b_{11}c_{11}$ ,第二个因子张量左上角位置元素为$a_{21}b_{21}c_{21}$ ,以此类推，我们就可以知道$\mathcal{X}_{111} = \sum^R_ra_{r1}b_{r1}c_{r1}$ ,张量$\mathcal{X}$ 其他位置的元素也可以用同样的犯法推导。

 $x_{i j k} \approx \sum_{r=1}^{R} a_{i r} b_{j r} c_{k r} \text { for } i=1, \ldots, I, j=1, \ldots, J, k=1, \ldots, K$

### 3.cp分解分解卷积核

$K(i,j,s,t)=\sum_{r=1}^{R} K_{r}^{x}(i) K_{r}^{y}(j) K_{r}^{s}(s) K_{r}^{t}(t)$

假设卷积层的输入为$X(i,j,s):$

则经过卷积的输出为：

$\begin{array}{l}
V(x, y, t)=\sum_{i} \sum_{j} \sum_{s} K(i, j, s, t) X(x-i, y-j, s) \\
=\sum_{r} \sum_{i} \sum_{j} \sum_{s} K_{r}^{x}(i) K_{r}^{y}(i) K_{r}^{s}(s) K_{r}^{t}(t) X(x-i, y-j, s) \\
=\sum_{r} K_{r}^{t}(t) \sum_{i} \sum_{j} K_{r}^{x}(i) K_{r}^{y}(i) \sum_{s} K_{r}^{s}(s) X(x-i, y-j, s)
\end{array}$ 

根据上面推导出的公式，我们可以将一层卷积分解称如下几步：

1.先使用$K_r(s)$（1x1xS)的 point wise 卷积。将输入channels从S 降到R.

2.接下来使用$(K_r^x,K_r^y)$ 进行一层深度分离卷积（depthwise seperable convolutions）,和mobilenet不同的是空间位置上的卷积也是分离的无重叠的。

3.在执行一次$K_r(t)$ (1x1xT)的point wise 卷积，将维度从R升到T.

[使用交替最小二乘求解CP分解](https://zhuanlan.zhihu.com/p/356372799)

### 4.cp分解的代码实现（使用pytorch和tensorly）

```
def cp_decomposition_conv_layer(layer, rank):
    """ Gets a conv layer and a target rank, 
        returns a nn.Sequential object with the decomposition """

    # Perform CP decomposition on the layer weight tensorly. 
    last, first, vertical, horizontal = \
        parafac(layer.weight.data, rank=rank, init='svd')

    pointwise_s_to_r_layer = torch.nn.Conv2d(in_channels=first.shape[0], \
            out_channels=first.shape[1], kernel_size=1, stride=1, padding=0, 
            dilation=layer.dilation, bias=False)

    depthwise_vertical_layer = torch.nn.Conv2d(in_channels=vertical.shape[1], 
            out_channels=vertical.shape[1], kernel_size=(vertical.shape[0], 1),
            stride=1, padding=(layer.padding[0], 0), dilation=layer.dilation,
            groups=vertical.shape[1], bias=False)

    depthwise_horizontal_layer = \
        torch.nn.Conv2d(in_channels=horizontal.shape[1], \
            out_channels=horizontal.shape[1], 
            kernel_size=(1, horizontal.shape[0]), stride=layer.stride,
            padding=(0, layer.padding[0]), 
            dilation=layer.dilation, groups=horizontal.shape[1], bias=False)

    pointwise_r_to_t_layer = torch.nn.Conv2d(in_channels=last.shape[1], \
            out_channels=last.shape[0], kernel_size=1, stride=1,
            padding=0, dilation=layer.dilation, bias=True)

    pointwise_r_to_t_layer.bias.data = layer.bias.data

    depthwise_horizontal_layer.weight.data = \
        torch.transpose(horizontal, 1, 0).unsqueeze(1).unsqueeze(1)
    depthwise_vertical_layer.weight.data = \
        torch.transpose(vertical, 1, 0).unsqueeze(1).unsqueeze(-1)
    pointwise_s_to_r_layer.weight.data = \
        torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1)
    pointwise_r_to_t_layer.weight.data = last.unsqueeze(-1).unsqueeze(-1)

    new_layers = [pointwise_s_to_r_layer, depthwise_vertical_layer, \
                    depthwise_horizontal_layer, pointwise_r_to_t_layer]
    
    return nn.Sequential(*new_layers)
```

### 5.Tucker 分解

[深入理解tucker分解和cp分解](https://zhuanlan.zhihu.com/p/302453223)

#### 1.Tucker分解

**张量的Tucker分解本质就是将原始张量分解维一个核心张量以及对应不同维度的因子矩阵（三阶张量分解中就是三个因子矩阵），每个因子矩阵我们可以看作对应于不同维度的线性变换操作** 

$X \approx \mathcal{G} \times{ }_{1} \mathbf{A} \times{ }_{2} \mathbf{B} \times{ }_{3} \mathbf{C}=\sum_{p=1}^{P} \sum_{q=1}^{Q} \sum_{r=1}^{R} g_{p q r} \mathbf{a}_{p} \circ \mathbf{b}_{q} \circ \mathbf{c}_{r}=[[ \mathcal{G} ; \mathbf{A}, \mathbf{B}, \mathbf{C} ]].$  

如果 $P,Q,R$小于$I,J,K$的话，**核心张量就相当于原始张量的压缩形式，即对原始张量进行降维操作的结果。因此，Tucker分解的确是可以看作PCA或者SVD降维的高阶形式。** 

#### 2.Tucker分解卷积核

我们考虑一个四维的卷积核，那么这个卷积核的Tucker分解可以表示为：

$K(i, j, s, t)=\sum_{r_{1}=1}^{R_{1}} \sum_{r_{2}=1}^{R_{2}} \sum_{r_{3}=1}^{R_{3}} \sum_{r_{4}=1}^{R_{4}} \sigma_{r_{1} r_{2} r_{3} r_{4}} K_{r 1}^{x}(i) K_{r 2}^{y}(j) K_{r 3}^{s}(s) K_{r 4}^{t}(t)$ 

在CP分解中，$K_r^x(i)K_r^y(j)$ 在空间方向上的卷积，并没有提高多少效率，所以Tucker分解有一个很好的性质，就是它不必在所有方向上都分解，我们可以进行一个mode-2的分解：

$K(i, j, s, t)=\sum_{r_{3}=1}^{R_{3}} \sum_{r_{4}=1}^{R_{4}} \sigma_{i j r_{3} r_{4}}(j) K_{r 3}^{s}(s) K_{r 4}^{t}(t)$

所以使用tucker分解后的神经网络推理过程可以分解为：

$V(x, y, t)=\sum_{i} \sum_{j} \sum_{s} K(i, j, s, t) X(x-i, y-j, s)$

$V(x, y, t)=\sum_{i} \sum_{j} \sum_{s} \sum_{r_{3}=1}^{R_{3}} \sum_{r_{4}=1}^{R_{4}} \sigma_{(i)(j) r_{3} r_{4}} K_{r 3}^{s}(s) K_{r 4}^{t}(t) X(x-i, y-j, s)$

$V(x, y, t)=\sum_{i} \sum_{j} \sum_{r_{4}=1}^{R_{4}} \sum_{r_{3}=1}^{R_{3}} K_{r 4}^{t}(t) \sigma_{(i)(j) r_{3} r_{4}} \sum_{s} K_{r 3}^{s}(s) X(x-i, y-j, s)$

那使用tucker分解之后的卷积过程应该是：

1.先使用Point wise卷积将通道数从S降到$R_3$ 

2.使用$\sigma(i)(j)r_3r_4$ ,将原来输入通道是S，输出通道是T的卷积转换为输入通道是$R_3$输出通道是$R_4$的卷积。如果$R_3$ $R_4$小于$S$或者$T$ ,就对网络产生了压缩效果。

3.再使用pointwise 卷积$K_{r4}^{t}(t)$ 将通道数升到T.

**怎么评估$R_3$和$R_4$的值？**

使用[variational Bayesian matrix factorization (VBMF)](http://www.jmlr.org/papers/volume14/nakajima13a/nakajima13a.pdf) 

 

代码：

```python
def tucker_decomposition_conv_layer(layer,ranks):
    """ Gets a conv layer, 
        returns a nn.Sequential object with the Tucker decomposition.
        The ranks are estimated with a Python implementation of VBMF
        https://github.com/CasvandenBogaard/VBMF
    """
    core, [last, first] = \
        partial_tucker(layer.weight.data, \
            modes=[0, 1], rank=ranks,n_iter_max=100,init='svd', verbose=True,tol=10e-6)

    # A pointwise convolution that reduces the channels from S to R3
    first_layer = torch.nn.Conv2d(in_channels=first.shape[0], \
            out_channels=first.shape[1], kernel_size=1,
            stride=1, padding=0, dilation=layer.dilation, bias=False)

    # A regular 2D convolution layer with R3 input channels 
    # and R3 output channels
    core_layer = torch.nn.Conv2d(in_channels=core.shape[1], \
            out_channels=core.shape[0], kernel_size=layer.kernel_size,
            stride=layer.stride, padding=layer.padding, dilation=layer.dilation,
            bias=False)

    # A pointwise convolution that increases the channels from R4 to T
    last_layer = torch.nn.Conv2d(in_channels=last.shape[1], \
        out_channels=last.shape[0], kernel_size=1, stride=1,
        padding=0, dilation=layer.dilation, bias=False)

    if (layer.bias) is not None:
        last_layer.bias.data = layer.bias.data

    first_layer.weight.data = \
        torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1)
    last_layer.weight.data = last.unsqueeze(-1).unsqueeze(-1)
    core_layer.weight.data = core

    new_layers = [first_layer, core_layer, last_layer]
    return nn.Sequential(*new_layers)

```

