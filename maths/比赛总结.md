比赛总结

[从最优化的角度看softmax](https://zhuanlan.zhihu.com/p/45014864) smooth 化不仅可以让优化更通畅 而且变相的在类间引入了一定的间隔

[Softmax理解之二分类与多分类](https://zhuanlan.zhihu.com/p/45368976)  softmax 相当于训练了多个二分类器

[Softmax理解之Smooth程度控制](https://zhuanlan.zhihu.com/p/49939159) softmax的输入应该合适 不应该过大或者过小

[Softmax理解之margin](https://zhuanlan.zhihu.com/p/52108088) margin在度量任务中比在分类任务中更重要

[Label Smoothing分析](https://zhuanlan.zhihu.com/p/302843504)

[Large-Margin Softmax Loss](https://blog.csdn.net/u014380165/article/details/76864572)

[Angular softmax loss](https://zhuanlan.zhihu.com/p/95447796)

[极大似然估计和交叉熵的关系](https://zhuanlan.zhihu.com/p/165139520)

[softmax函数和sigmoid函数的区别与联系](https://zhuanlan.zhihu.com/p/356976844)

[逻辑回归与softmax回归](https://zhuanlan.zhihu.com/p/62381502)

[sphereface详解](https://blog.csdn.net/u014380165/article/details/76946380)

[损失函数改进之large-margin softmax loss](https://blog.csdn.net/u014380165/article/details/76864572)

[人脸识别：arcFace Loss详解](https://blog.csdn.net/wfei101/article/details/82530692)

[人脸识别损失函数总结](https://zhuanlan.zhihu.com/p/59440497)

[ResNeXt](https://zhuanlan.zhihu.com/p/32913695)

[人脸识别损失函数简介及pytorch实现](https://zhuanlan.zhihu.com/p/60747096)

[ArcFace算法笔记](https://blog.csdn.net/u014380165/article/details/80645489)

[上采样方法及其对比](https://www.zhihu.com/question/328891283)

#### 1.softmax的定义及其作用

$$
\operatorname{Softmax}\left(z_{i}\right)=\frac{e^{z_{i}}}{\sum_{c=1}^{C} e^{z_{c}}}
$$

其中$z_i$为第i个节点的输出值，C为输出节点的个数，即分类的类别个数。通过softmax函数可以将多分类的输出值转换为范围在[0,1]和为1的概率分布。

#### 2.softmax与sigmoid的关系

(1)sigmoid可以用在多标签分类问题（比如歌曲分类，（男声，吉他，钢琴），类别之间不互斥）。

sigmoid函数是一种logistic函数，他将任意实数值转换到[0,1]之间，如图1所示，函数表达式为：
$$
\operatorname{Sigmoid}(x)=\frac{1}{1+e^{-x}}
$$
它的导函数为：
$$
\text { Sigmoid }^{\prime}(x)=\operatorname{Sigmoid}(x) \cdot(1-\operatorname{Sigmoid}(x))
$$
<img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/image-20210616163414597.png" style="zoom:50%;" />



​                                                                                        **图（1）**

(2)softmax 用来解决多分类问题，只有一个正确答案，互斥输出。softmax函数的分母综合了原始输出值的所有因素，所以softmax函数得到的不同概率之间相互关联。

softmax函数是二分类函数Sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展示出来。对于二分类问题来说，理论上两者没有本质区别，对于二分类而言（以输入$x_1$为例）:
$$
\begin{array}{l}
\text { Sigmoid函数: } \quad \operatorname{output}\left(x_{1}\right)=\frac{1}{1+e^{-x_{1}}}(1)\\
\text { Softmax函数: } \quad \text { output }\left(x_{1}\right)=\frac{e^{x_{1}}}{e^{x_{1}}+e^{x_{2}}}=\frac{1}{1+e^{-\left(x_{1}-x_{2}\right)}}(2)
\end{array}
$$
虽然可以化为相同的形式，但是两者还是存在差异的，sigmoid的输出可以理解成分类成目标类别的概率P,而不分类到该类别的概率是1-P.sofmax是对两个类建模，输出的是两个类分别的概率，并且概率和为1。

#### 3.softmax与逻辑回归的关系

标准的softmax函数定义如下：
$$
\operatorname{softmax}(i, \mathbf{Z})=\frac{e^{z_{i}}}{\sum_{j=1}^{K} e^{z_{j}}}, i=1, \cdots, K \text { and } \mathbf{Z}=\left(z_{1}, \ldots, z_{K}\right)^{T} \in \mathbb{R}^{K}
$$

$$
P\left(Y_{i}=c\right)=\operatorname{softmax}\left(c,\left[\boldsymbol{w}_{1}^{T} \mathbf{x}_{i}, \cdots, \boldsymbol{w}_{K}^{T} \mathbf{x}_{i}\right]\right)
$$

那softmax和我们在multi-nominal logistic regression 部分得到的模型是不是等价的呢？

首先我们有如下关于softmax函数的旋转不变性质：
$$
\begin{aligned}
\frac{e^{\left(\boldsymbol{w}_{c}+C\right) \cdot \mathbf{x}_{i}}}{\sum_{k=1}^{K} e^{\left(\boldsymbol{w}_{k}+C\right) \cdot \mathbf{x}_{i}}} &=\frac{e^{\boldsymbol{w}_{c} \cdot \mathbf{x}_{i}} e^{C \cdot \mathbf{x}_{i}}}{\sum_{k=1}^{K} e^{\boldsymbol{w}_{k} \cdot \mathbf{x}_{i}} e^{C \cdot \mathbf{x}_{i}}} \\
&=\frac{e^{C \cdot \mathbf{x}_{i}} e^{\boldsymbol{w}_{c} \cdot \mathbf{x}_{i}}}{e^{C \cdot \mathbf{x}_{i}} \sum_{k=1}^{K} e^{w_{k} \cdot \mathbf{x}_{i}}} \\
&=\frac{e^{\boldsymbol{w}_{c} \cdot \mathbf{x}_{i}}}{\sum_{k=1}^{K} e^{w_{k} \cdot \mathbf{x}_{i}}}
\end{aligned}
$$
那么，取
$$
\begin{aligned}
\boldsymbol{w}_{1}^{\prime} &=\boldsymbol{w}_{1}-\boldsymbol{w}_{K} \\
\cdots & \cdots \\
\boldsymbol{w}_{K-1}^{\prime} &=\boldsymbol{w}_{K-1}-\boldsymbol{w}_{K} \\
\boldsymbol{w}_{K}^{\prime} &=0
\end{aligned}
$$
因此：
$$
P\left(Y_{i}=c\right)=\frac{e^{\boldsymbol{w}_{c}^{T} \mathbf{x}_{i}}}{\sum_{k=1}^{K} e^{\boldsymbol{w}_{k}^{T} \mathbf{x}_{i}}}, c=1, \cdots, K
$$
可以转换成：
$$
\begin{array}{c}
P\left(Y_{i}=1\right)=\frac{e^{\left(\boldsymbol{w}_{1}^{\prime}\right)^{T} \mathbf{x}_{i}}}{1+\sum_{k=1}^{K-1} e^{\left(\boldsymbol{w}_{k}^{\prime}\right)^{T} \mathbf{x}_{i}}} \\
\cdots \cdots \\
P\left(Y_{i}=K-1\right)=\frac{\left.e^{\left(\boldsymbol{w}_{K-1}^{\prime}\right.}\right)^{T} \mathbf{x}_{i}}{1+\sum_{k=1}^{K-1} e^{\left(\boldsymbol{w}_{k}^{\prime}\right)^{T} \mathbf{x}_{i}}} \\
P\left(Y_{i}=K\right)=\frac{1}{1+\sum_{k=1}^{K-1} e^{\left(\boldsymbol{w}_{k}^{\prime}\right)^{T} \mathbf{x}_{i}}}
\end{array}
$$

#### 4.交叉熵和极大似然估计的再理解

对于一个多分类问题（假设为K类），有数据集$D=\{(x_i,y_I)|i\}$。我们希望建立模型取建模概率分布

$p_{\theta}(y|x)$，模型参数为$\theta$ 。

我们使用损失函数评估模型的好坏，可以采用两种方式来导出。

**极大似然估计**

由于是多分类问题，故样本空间上的p(y|x)满足某个categorical distribution.由categorical distribution定义知,
$$
p(y \mid x ; p)=\prod_{k=1}^{K} p_{k}^{y_{k}}
$$
其中，p是分布的参数，也是分布的输出概率向量。y是one-hot编码的标签向量。

例如对于一个天气4分类问题，输出概率向量如下：

p={"rain":.14,'snow':.37,'sleet':.03,'hail':.46}

则分类为snow的概率为$\operatorname{Pr}(\mathrm{y}=\text { snow }=[0,1,0,0])=.14^{0} .37^{1} .03^{0} * .46^{0}=0.37$

我们使用极大似然估计去估计分布参数p.

假设有n个样本：$(x^{(i)},y^{(i)})$ ,则似然函数为：
$$
\mathcal{L}\left(\left(x^{(i)}, y^{(i)}\right) ; p\right)=\prod_{i=1}^{n} \prod_{k=1}^{K} p_{k}^{y_{k}}
$$
我们期望最大化似然估计，即最小化负对数似然函数：
$$
\min -\sum_{i=1}^{n} \sum_{k=1}^{K} y_{k} \log p_{k}
$$
由于采用one-hot编码，故$y_k=0$的项的乘积均为0，只需考虑$y_k=1$时。故上述函数可变形为：
$$
\min -\sum_{i=1}^{n} \log P_{k}\left(y_{k}=1\right)
$$
**交叉熵**

信息论背景知识补习：

信息一般可以被表述为不确定性的程度，有如下特性

- 一定发生的事件没有信息
- 很有可能发生的事件几乎没有信息
- 随机事件拥有更多的信息
- 独立事件可以增加信息-抛两次骰子的信息量大于抛一次正面骰子的信息量

事件的信息可以形式化为：
$$
I(x)=-\log (P(x))
$$
熵用于衡量信息的多少，被定义为：
$$
H(x)=\mathbb{E} x \sim P(I(x))=-\mathbb{E} x \sim p[\log P(x)]
$$
离散随机变量$x$的熵即：
$$
H(x)=-\sum_{x} P(x) \log P(x)
$$
若log 以2为底，则可以衡量编码信息的比特数多少。在信息论中，信息与随机性是正相关的。高熵等于高随机性，需要更多的比特来编码。

例如,计算丢一枚硬币的熵：
$$
H(x)=-p(\text { 正面 }) \log _{2} p(\text { 正面 })-p(\text { 反面 }) \log _{2} p(\text { 反面 })=-\log _{2} \frac{1}{2}=1
$$
则我们可以用1位比特来编码。

KL Divergence 常用于衡量两个分布P,Q的距离，被定义为
$$
D_{K L}(P \| Q)=\mathbb{E}_{x \sim P} \log \frac{P(x)}{Q(x)}
$$
故对于离散型随机变量而言，
$$
D_{K L}(P \| Q)=\sum_{x} P(x)(\log P(x)-\log Q(x))
$$
注意，KL距离并不对称。

熵可以衡量编码信息的最少比特数，交叉熵则可以衡量使用Q的错误优化编码方案对具有分布P的x进行编码的最小比特数。其被定义为：
$$
H(P, Q)=-\sum_{x} P(x) \log Q(x)
$$
由于H(P)与模型参数无关，可以视为常数。故最小化KL距离等价于最小化交叉熵。

在深度学习中，P一般为真实标签的分布，Q一般为模型预测输出的分布。

**交叉熵损失函数**
$$
-\sum_{y} P(y \mid x) \log Q(\hat{y} \mid x)=-\sum_{k=1}^{K} y_{k} \log \hat{y_{k}}
$$
由此可见，最小化交叉熵和最小化负对数似然函数是等价的。

#### 4.从概率角度解释softmax交叉熵损失函数

最大化对数似然函数和最小化交叉熵是等价的

#### 5.softmax与crossentropy结合组成softmax loss

softmax loss 过程如下图所示：

![img](https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/v2-7cb3f114d51b6a129d0075c06c9cb20b_720w.jpg)

符号定义如下：

1.输入为**z**向量，$z=[z_1,z_2,z_3,..z_n]$,维度为（1，n）

2.经过softmax函数，$a_{i}=\frac{e^{z_{i}}}{\sum_{k=1}^{n} e^{z_{k}}}$

可得输出*a*向量，$a=\left[\frac{e^{z_{1}}}{\Sigma_{k=1}^{n} e^{z_{k}}}, \frac{e^{z_{2}}}{\Sigma_{k=1}^{n} e^{z_{k}}}, \ldots, \frac{e^{z_{n}}}{\Sigma_{k=1}^{n} e^{z_{k}}}\right]$,维度为（1，n）

3.softmax loss 损失函数定义为L,$L=-\Sigma_{i=1}^{n} y_{i} \ln \left(a_{i}\right)$,L是一个标量，维度为（1,1）

其中y向量为模型的label,维度也是（1，n）,为已知量，一般为onehot形式。

我们假设第J个类别是正确的，则y = [0,0,---,1,..0],只有$y_j = 1$,其余$y_i = 0$ 

那么$L=-y_{j} \ln \left(a_{j}\right)=-\ln \left(a_{j}\right)$



我们的目标是求标量 L 对向量z的导数 $\frac{\partial L}{\partial \boldsymbol{z}}$.

由链式法则，$\frac{\partial L}{\partial \boldsymbol{z}}=\frac{\partial L}{\partial \boldsymbol{a}} * \frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}$ 其中a和z均为维度为（1，n）的向量。

标量对向量求导规则如下：

![img](https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/v2-f51ba67cf81e744488714c771e9325aa_720w.jpg)

可知，标量对向量求导，维度不变，维度也为（1，n）。



而向量a对向量z求导，可得Jacobian矩阵：

![img](/home/xy/pan/xy_workspace/git_workspace/notebook/maths/%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93.assets/v2-cffefd741292a0e15f66d0e3c307c6f9_720w.jpg)

其中f对应的就是softmax函数，x对应为z.可知向量a对向量z求导是一个jacobian矩阵，维度为（n,n）.

1.求$\frac{\partial L}{\partial \boldsymbol{a}}$

由$L=-y_{j} \ln \left(a_{j}\right)=-\ln \left(a_{j}\right)$,可知最终的loss只跟$a_j$有关。

$\frac{\partial L}{\partial \boldsymbol{a}}=\left[0,0,0, \ldots,-\frac{1}{a_{j}}, \ldots, 0\right]$

2.求$\frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}$

a 是一个向量，z也是一个向量，则$\frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}$ 是一个jacobian矩阵，类似这样：



<img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/v2-ec8b04ca0ee923686c9f36277ff8b7b1_720w.jpg" alt="img" style="zoom: 80%;" />

可以发现其实jacobian矩阵的每一行对应$\frac{\partial a_{i}}{\partial \boldsymbol{z}}$ .

由于$\frac{\partial L}{\partial \boldsymbol{a}}$ 只有第j列不为0，由矩阵乘法，其实我们只要求$\frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}$ 的第j行，也即 $\frac{\partial a_{j}}{\partial \boldsymbol{z}}$ ,

$\frac{\partial L}{\partial \boldsymbol{z}}=-\frac{1}{a_{j}} * \frac{\partial a_{j}}{\partial \boldsymbol{z}}, \text { 其中 } a_{j}=\frac{e^{z_{j}}}{\Sigma_{k=1}^{n} e^{z_{k}}}$ 。

（1）当$i \neq j$ 

$\begin{array}{l}
\frac{\partial a_{j}}{\partial z_{i}}=\frac{0-e^{z_{j}} e^{z_{i}}}{\left(\sum_{k}^{n} e^{z_{k}}\right)^{2}}=-a_{j} a_{i} \\
\frac{\partial L}{\partial z_{i}}=-a_{j} a_{i} *-\frac{1}{a_{j}}=a_{i}
\end{array}$ 

(2)当$i=j$ 

$\begin{array}{l}
\frac{\partial a_{j}}{\partial z_{j}}=\frac{e^{z_{j}} \sum_{k}^{n} e^{z_{k}}-e^{z_{j}} e^{z_{j}}}{\left(\sum_{k}^{n} e^{z_{k}}\right)^{2}}=a_{j}-a_{j}^{2} \\
\frac{\partial L}{\partial z_{j}}=\left(a_{j}-a_{j}^{2}\right) *-\frac{1}{a_{j}}=a_{j}-1 \\
\text { 所以, } \quad \frac{\partial L}{\partial \boldsymbol{z}}=\left[a_{1}, a_{2}, \ldots, a_{j}-1, \ldots a_{n}\right]=\boldsymbol{a}-\boldsymbol{y} \circ
\end{array}$



softmax cross entropy loss的求导结果非常优雅，就等于预测值与label的差。

