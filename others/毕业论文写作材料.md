## 1.Conversion of Continuous-valued Deep Networks to Efficient Event-Driven Networks for Image Classification  翻译

深度神经网咯（ANN），比如GoogleNet（Szegedy et al,.), Inceptionnet, resNet, VGG-16(Simomyan and ziserman)已经将ImageNet的错误率降低到了一个新的水平，这些大型网络的推导，需要大量的计算和功耗，这限制了它们在移动终端以及嵌入式设备上的应用。

​      最近的研究工作表明，SNN中事件驱动的计算方式有希望减少大型神经网络的推导耗时，并降低功耗，使深度神经网络的实时推导成为可能。 （Farabet et al., O'Connor et al., Neil et al., Zambrano and Bohte).深度SNN在输出层的第一脉冲信号产生时就可以对结果进行预测，而ANN则需要所有层的全部计算完成才能得出结果 。SNN天生适合处理时间驱动的传感器数据（Posch et al., 2014）甚至在经典的frame-based 机器视觉应用中 ，比如目标检测和识别，它们也表现出了快速，精准，高效的特点，特别是实现在神经形态硬件平台上时（Neil and Liu; StromQatias; Esser;）因此SNNs可以在一些对实时性要求较高的任务中替代深度ANNs，比如在大型移动场景中的目标检测，追踪，动态识别（activity recognition）等。

目前 ，已经有SNN模型被实现在了数字硬件平台上，比如FPGAs(Neil and Liu, Gokhale),但是有着数万神经元的的脉冲神经网络可以被实现在大规模神经形态平台上 ，比如TrueNorth(Benjamin; Merolla) 和 SpiNNaker(Furber;),最新的关于TrueNoth的实验表明，包含数百万神经元的CNN模型，可以被实现在一套功耗只有几百mw的芯片上 并且保持较高的准确率，这使得深度脉冲神经网络和基于事件驱动的传感器相互结合用来处理实际应用的部署成为可能。（Orchard， Serrano-Gotarredona, Kiselev）.

为了结合连续值型神经网络和离散型脉冲神经网络的优势，有必要寻找方法使得脉冲神经网络的精度和对应深度的连续型神经网络相媲美。这样的方法大致分为两类，一种是直接训练SNN的方法，用反向传播训练算法训练SNN（Lee et al., 2016）, 在SNN分类层使用随机梯度下降法（Stromatias et al., 2017）或者在训练过程中修改ANN的激活函数使网络参数更好的映射到SNN上。（O‘Connor et al,2013; Esser et al., 2015; Hunsberger and Eliasmith(2016)）,这种方法是基于AlexNet的（Krizhevsky et al., 2012）.虽然结果可以和相同结构的ANN相媲美，但是还是没有达到训练像VGG-16这样结构的ANN的水平。

一个更直接的方法是将一个预训练的ANN的参数直接映射到相同结构的 SNN上，并使它达到和ANN一样的精度。关于预训练ANN到SNN的研究工作始于Perez-Carrasco et al.(2013), CNN 中的基本单元被替换为具有泄漏（leak）和静息时间（refractory periods）的生物神经元模型，目的是用它来处理基于事件的传感器信息。Cao et al.(2015) 提出了生物神经元模型（IF）和现在通用的ANN神经元模型ReLU的基本联系，他们将CNN每层的偏置设置为0，并将下采样层都设置为平均采样，并在一些简单的机器视觉任务中取得了和ANN 近似的结果。他们的方法被Diehl et al.(2015)进行了改进，并转换得到的SNN在MNIST（LeCun et al., 1998）上取得了几乎无损的精度。他们采用权值归一化的方法去避免神经元脉冲发放过多或不足，从而避免转化过程中精度的损失。Humsberger and Eliasmith(2016) 在训练过程中引入了噪音，从而提高了CNN到SNN转换过程的健壮性，并且采用了更具有生物性的神经元模型。Esser er al.(2016) 为 为TrueNorth 平台设计的SNN模型具有二值化的权值和限制性的连接。Zambrano and Bohte(2016)提出了一种转换方法，可以调整SNN的神经元的脉冲发放频率来控制脉冲序列所携带的信息量。

这些方法在MNIST数据集上取得了非常不错的结果，但是当SNN的网络结构升级到可以解决更复杂任务比如，CIFAR-10(Krizhevsky,2009)，imagenet,的CNN结构时，它的结果就没有那么理想了。一个重要的原因是SNN中的很多操作对转换精度的提升有着重要的作用，比如max-pooling 层，softmax函数，batch-normalization操作，之前的一些工作都没有实现这些操作，只是转换了基本的CNN结构，导致精度损失，所以之前的方法都不能用来将任意预训练的CNN模型转化为SNN。

在本工作中，我们 指出了当前的CNN到SNN转化方法的一些重要缺点，通过对神经元输出频率和与其对应的ANN中的神经元的激活函数的输出值进行数学分析，我们可以推导出之前的转化方法中产生的误差的理论度量。基于这个创新的理论，我们修改了脉冲神经元模型，大大的提升了SNN的表现。通过实现脉冲版本的max-pooling层,softmax 激活函数，神经元偏置，和batch normalization(Ioffe and Szegedy,2015)，我们扩大了可以转化的CNN模型的范围，我们首次实现了GoogleNet Inception-V3的SNN版本，更进一步，我们证明了CNN到SNN的转化方法可以跟现有的网络压缩方法，比如权值量化，低精度激活值，进行协同。nh

