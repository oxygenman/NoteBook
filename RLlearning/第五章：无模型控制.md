---
typora-copy-images-to:ipic

---



#                      第五章：无模型控制

- 上节课：无模型预测 指的是评估一个无模型MDP的价值函数

- 本节课：无模型控制 指的是评估过后进而对策略进行优化

- ## on-policy Monte-Carlo Control

- ## 回顾

  在有模型的MDP中，我们通过策略迭代计算状态价值，然后用贪婪算法来优化策略，可以找到最优的策略，但是在无模型控制中，我们不知道状态转移概率矩阵，进而无法确定在当前状态下采取怎样的行为更合适。解决这一问题的方法是，使用状态行为对下的价值Q（s,a）来代替状态价值，这样只要知道某个状态的动作空间就可以。

  但是这样的话，至少还存在一个问题，那就是根据贪婪算法来改进策略时，有些路可能因为采样不足，而导致这条路被忽略，而再也不走这条路。

  所以我们要采用$\epsilon-Greedy$ Exploration

- ## $\epsilon-Greedy$ Exploration

- $\epsilon$的几率随机选择动作

  ![屏幕快照 2017-11-27 下午3.05.32](https://ws1.sinaimg.cn/large/006tNc79ly1flwnrdq2byj319m09wtad.jpg)

- 蒙特卡罗控制的过程

![屏幕快照 2017-11-27 下午3.11.06](https://ws2.sinaimg.cn/large/006tNc79ly1flwnw00lzfj318k0q4tc3.jpg)

每个回合：

策略迭代：蒙特卡罗策略评估，我们根据上节的蒙特卡罗评估方法，找出Q值，像以前讲的有模型的策略评估一样，这里我们没必要进行很深的迭代直到它收敛，而只需要N步即可，

策略优化：采用$\epsilon-greedy$ 进行策略优化

- 图中每一个箭头都对应着多个Episode.也就是说我们一般在经历了多个Episode之后才进行一次策略改善，实际上我们可以在每经历一个Episode之后就改善策略。但是不管使用哪种方式，在$\epsilon-greeedy$ 下，我们始终只能带的基于某一策略下的近似Q函数，且该算法没有一个终止条件，因为它一直在进行探索。因此我们必须关注以下两个方面：一方面我们不想丢掉任何更好的信息和状态，另一方面我们希望随着我们的策略改善，我们最终希望能终止于某一个最优策略，因为最优策略不应该包括一些随机行为选择。为此引入：GLIE

- ## GLIE（Greedy in the limit with infinite Exploration）,

  有限制的无限贪婪探索，即$\epsilon$是随着探索次数的增加是可变的，是趋向于0的。例如我们取$\epsilon=1/k$,那么称$\epsilon$贪婪蒙特卡罗控制就具备GLIE特性。

  - GLIE 蒙特卡罗控制的过程：

    - 抽样采取策略$\pi$的的k个episode:$\{S_1,A_1,R_2,…,S_T\}\sim\pi$

    - 对于每一个episode

      $N(S_t,A_t)\leftarrow N(S_t,A_t)+1$

      $Q(S_t,A_t)\leftarrow Q(S_t,A_t) + \frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t))$

    - 改善策略

      $\epsilon\leftarrow1/k$

      $\pi\leftarrow\epsilon-greedy(Q)$

  - 21点的最优策略

    ## ![屏幕快照 2017-11-27 下午4.11.39](https://ws1.sinaimg.cn/large/006tNc79ly1flwpmtlh40j31eu108nfk.jpg)

  ## MC vs TD Control

  在控制过程中用TD代替MC

  - 用TD来评估Q（S，A）

  - 用$\epsilon - greedy$ 来优化策略

  - 每一个时间步都进行更新

    ## SARSA

    ![屏幕快照 2017-11-27 下午4.36.54](https://ws4.sinaimg.cn/large/006tNc79ly1flwqckuk85j31bc0vg410.jpg)

当一个Agent处于某一个状态S，在这个状态下它可尝试各种不同的行为，当遵循某一策略时，会根据当前策略选择一个行为A，个体实际执行这个行为，与环境发生交互，环境会根据其行为给出即时奖励R，并且进入下一个状态$S'$,在这个后续状态$\S'$,再次遵循当前策略，产生一个行为$A'$, 此时，个体并不执行该行为，而是通过自身当前状态行为价值函数带到该$(S'A')$状态行为对的价值，利用该价值同时结合$Q(S,A)$和R 来更新$Q(S,A)$.

SARSA每一步都采用$\epsilon-greedy$策略。进行动作的选择。

- $Q(s,a)$收敛到最优策略状态值$q_*(s,a)$ ,需要满足两个条件：
  条件1：任何时候的策略$\pi_t(a|s)$符合GLIE特性；
- 步长系数$\alpha_t$ 满足：$\sum_{i=1}^\infty \alpha_t=\infty$且$\sum_{t=1}^\infty {\alpha_t}^2<\infty$
- ​