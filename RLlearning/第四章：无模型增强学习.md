---
typora-copy-images-to:ipic

---







#                  第四章：无模型增强学习



## 蒙特卡罗增强学习：

## 蒙特卡罗增强学习的特点：

- MC从所经历过的回合（episodes）学习
- MC是无模型的：没有状态转移矩阵和奖励信息
- MC采用最简单的思想：value=mean return
- 警告：MC 只能用在具有完整episod的马尔科夫决策过程中 ，即每一个状态动作序列必须有终止过程


## 蒙特卡罗策略评估：

- 目标：从遵从某个策略$\pi$的经验片段，学习到在这个模型中所有状态的价值$v_\pi$

​     数学描述：$S_1, A_1, R_2, …, S_k\sim\pi$

- $t$ 时刻状态$S_t$ 总的有衰减的奖励收获为：

​     $G_t = R_{t+1} +\gamma R_{t+2} + … +\gamma ^{T-1}R_T$

​     其中T为终止时刻

- 价值函数为：

  $v_\pi (s) = \mathbb{E}_\pi[G_t|S_t = s]$

- 蒙特卡罗策略评估用经验收获代替期望收获

  ### 在状态转移的过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，即存在有回路，我们应该怎么计算收获，可以有以下两种方法：

  - ## 首次访问蒙特卡罗策略评估 

    即仅在第一次访问时列入计算

    遍历所有回合（episode）,每个episode中，当s第一次出现时

    $N(s)\leftarrow N(s) + 1$

    $S(s)\leftarrow S(s) + G_t$

    那么：$V(s) = S(s)/N(s)$,即取一个平均值

    根据大数定理，$V(s)\rightarrow v_\pi(s)  as N(s)\rightarrow \infty$

  - ## 每次访问蒙特卡罗策略评估

    即每次访问S都列入计算，后续步骤同上。

  - ## 例子：二十一点游戏

    - states
      - 我当前牌的总和 current sum (12-21)；低于12时，你可以安全的再叫牌，所以没有意义
      - 荷官所展示的牌（1-10）；庄家会显示一张牌面给玩家
      - 我是否有可用的 1?(yes-no)；A既可以当1点，也可以当11点。

    - Actions:
      - stick: 停止要牌，并且游戏终止
      - twist：再要一张牌

    - Reward for stick:
      - 牌面和大于庄家 +1
      - 牌面和等于庄家 0
      - 牌面和小于庄家-1

    - Reward for twist:
      - 如果爆掉，即牌面和>21,reward=-1,游戏终止
      - 其他 0

    - 状态转移：如果牌面和<12,自动twist。

      ![屏幕快照 2017-11-25 下午2.26.29](https://ws4.sinaimg.cn/large/006tKfTcly1flubhey8snj31hy10uwlu.jpg)

- ## 增量平均/累进更新平均值 incremental Mean

  ## 见David Silver ppt,蒙特卡罗学习方法有很多缺点，因此实际应用并不多，常用的是TD学习方法。

  ## 时序差分学习 Temporal-Difference Learning

  - 和蒙特卡罗学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习不完整的Episode,通过自身的引导（bootstraping）,猜测episode的结果，同时持续更新这个猜测。
  - 目标和蒙特卡罗是一样的，都是从采用策略$\pi$的经验中学习到 状态价值$v_\pi$
  - 它和蒙特卡罗的累进更新有点儿像
    - incremental mean 是从实际的经验中得到的$G_t$,来更新$V(S_t)$
    - ​
    - TD是用一个估计值$R_{t+1} + \gamma V(S_t+1)$来代替$G_t$
    - $R_{t+1} + \gamma V(S_t+1)$叫做TD target
    - $\delta _t =R_{t+1} + \gamma V(S_t+1)-V(S_t)$叫TD error
    - TD的采样是走一步，MC是一直走到结束

  ## 例子：开车回家

  ![屏幕快照 2017-11-25 下午10.05.46](https://ws4.sinaimg.cn/large/006tKfTcly1fluom7xj36j31kw0s7tjz.jpg)


这个例子中状态的价值是预测的总的回家时间，图中展示的仅仅是一个episode的计算，一个episode中对于每个状态价值的更新。

TD方法对初始值设置比较敏感

## Batch MC and TD

即在经验不足样本不足的情况下  MC和TD的对比




