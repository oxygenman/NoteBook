比赛总结

[从最优化的角度看softmax](https://zhuanlan.zhihu.com/p/45014864) smooth 化不仅可以让优化更通畅 而且变相的在类间引入了一定的间隔

[Softmax理解之二分类与多分类](https://zhuanlan.zhihu.com/p/45368976)  softmax 相当于训练了多个二分类器

[Softmax理解之Smooth程度控制](https://zhuanlan.zhihu.com/p/49939159) softmax的输入应该合适 不应该过大或者过小

[Softmax理解之margin](https://zhuanlan.zhihu.com/p/52108088) margin在度量任务中比在分类任务中更重要

[Label Smoothing分析](https://zhuanlan.zhihu.com/p/302843504)

[Large-Margin Softmax Loss](https://blog.csdn.net/u014380165/article/details/76864572)

[Angular softmax loss](https://zhuanlan.zhihu.com/p/95447796)

比赛中使用的数据增强策略：

```python
 train_transform = A.Compose(
        [
            A.RandomResizedCrop(height=height, width=width, scale=(0.8, 1.0), p=1.0),
            #随机裁剪
            A.OneOf([
                A.HueSaturationValue(hue_shift_limit=0.4, sat_shift_limit=0.4,
                                        val_shift_limit=0.4, p=0.9),
                #随机色调饱和度
                A.RandomBrightnessContrast(brightness_limit=0.3,
                                            contrast_limit=0.3, p=0.9),
            ],p=0.9), 
            # 亮度对比度增强
            A.GaussianBlur(p=0.5),
            #高斯模糊
            A.HorizontalFlip(p=0.5),
            #水平翻转
            A.VerticalFlip(p=0.5),
            #竖直翻转
            A.Transpose(p=0.5),  # TTA×8
            #转置
            A.Normalize(max_pixel_value=1.0, p=1.0),
            #标准化
            A.CoarseDropout(p=0.5, max_width=32, max_height=32),
            #随机抠方块
            # A.Cutout(p=0.5),
            ToTensorV2(p=1.0),
        ],
        p=1.0,
    )
```

#